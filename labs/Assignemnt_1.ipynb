{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brownfortress/NLU-2024-labs/blob/main/labs/04_neural_LM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you are on Colab\n",
    "# !python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import itertools\n",
    "import numpy as np\n",
    "DEVICE = 'cuda:0'  # it can be changed with 'cpu' if you do not have a gpu\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from model import MultiModel\n",
    "from model import NTAvSGD\n",
    "from model import V_Dropout\n",
    "from model import AverageOfGradientsSGD\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficiency purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN_cell, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(input_emb + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)#matrix vocabulary*dimension of the input\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)#sentence\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpus \n",
    "\n",
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line.strip() + \" \" + eos_token)\n",
    "    return output\n",
    "\n",
    "# Vocab with tokens to ids\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# We add two special tokens end of sentence and padding\n",
    "#The dataset was already cutoffed\n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class computes and stores our vocab \n",
    "# Word to ids and ids to word\n",
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "\n",
    "        for sentence in corpus:\n",
    "            # We get from the first token till the second-last token\n",
    "            self.source.append(sentence.split()[0:-1])\n",
    "            # We get from the second token till the last token\n",
    "            self.target.append(sentence.split()[1:])\n",
    "            # See example in section 6.2\n",
    "\n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "\n",
    "    # Auxiliary methods\n",
    "\n",
    "    # Map sequences of tokens to corresponding computed in Lang class\n",
    "    def mapping_seq(self, data, lang):\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    print('You have to deal with that')\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "    \n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(\n",
    "                            param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    \n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(DEVICE)\n",
    "    new_item[\"target\"] = target.to(DEVICE)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "# You can reduce the batch_size if the GPU memory is not enough\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=128, collate_fn=partial(\n",
    "    collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=partial(\n",
    "    collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exam Exercise\n",
    "## Part 1 (4 points)\n",
    "In this, you have to modify the baseline LM_RNN by adding a set of techniques that might improve the performance. In this, you have to add one modification at a time incrementally. If adding a modification decreases the performance, you can remove it and move forward with the others. However, in the report, you have to provide and comment on this unsuccessful experiment.  For each of your experiments, you have to print the performance expressed with Perplexity (PPL).\n",
    "<br>\n",
    "One of the important tasks of training a neural network is  hyperparameter optimization. Thus, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration (in particular <b>the learning rate</b>). \n",
    "These are two links to the state-of-the-art papers which use vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). \n",
    "\n",
    "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***).\n",
    "\n",
    "1. Replace RNN with a Long-Short Term Memory (LSTM) network --> [link](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)#while using SGD the learning rate could be above 0 to perform\n",
    "2. Add two dropout layers: --> [link](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "    - one after the embedding layer, \n",
    "    - one before the last linear layer\n",
    "3. Replace SGD with AdamW --> [link](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (11 points)\n",
    "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***) and it should be lower than the one achieved in Part 1.1 (i.e. base LSTM).\n",
    "\n",
    "Starting from the `LM_RNN` in which you replaced the RNN with a LSTM model, apply the following regularisation techniques:\n",
    "- Weight Tying \n",
    "- Variational Dropout (no DropConnect)\n",
    "- Non-monotonically Triggered AvSGD \n",
    "\n",
    "These techniques are described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor board logging system\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def log_values(writer, step, ppl, prefix):\n",
    "  writer.add_scalar(f\"{prefix}/ppl\", ppl, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def avg_train_loop(data, optimizer, criterion, model, VDROP, clip=5):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:#data is the list of batches\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "\n",
    "        if VDROP:\n",
    "            model.reset_dropout(sample[\"source\"])\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, zeroing the computational graph calculated by pytorch\n",
    "        # clip the gradient to avoid explosive gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step()  # Update the weights    weight_decay_range, patience_range, emb_drop_range,\n",
    "\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            #torch.cuda.empty_cache()\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        param_string,\n",
    "    n_epochs=100,\n",
    "    patience_set=3,\n",
    "    losses_train=[],\n",
    "    losses_dev=[],\n",
    "    sampled_epochs=[],\n",
    "    best_ppl=math.inf,\n",
    "    best_model=None,\n",
    "    modelM=None,\n",
    "    optimizer=None,\n",
    "    NM_ASGD=False,\n",
    "    ASGD_optim = NTAvSGD,\n",
    "    Logging_interval=1,\n",
    "    non_monotone_interval=5,\n",
    "    lr=.5,\n",
    "    VDROP=False,\n",
    "    clip=5,\n",
    "    criterion_train=None,\n",
    "    weight_decay_Av =.05,\n",
    "    criterion_eval = None,\n",
    "    exp_name=None,\n",
    "):\n",
    "\n",
    "\n",
    "    patience = patience_set\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{param_string}\")\n",
    "    pbar = tqdm(range(1, n_epochs))\n",
    "    # If the PPL is too high try to change the learning rate\n",
    "    loss_log = []\n",
    "    T = 0\n",
    "    t = 0\n",
    "    for epoch in pbar:\n",
    "        loss = avg_train_loop(train_loader, optimizer,\n",
    "                              criterion_train, modelM, VDROP, clip)\n",
    "        # The epoch is treated as the k in NT-AvSGD\n",
    "        if epoch % 1 == 0:\n",
    "            sampled_epochs.append(epoch)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, modelM)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            # 4-9: of NT-AvSGD\n",
    "            if NM_ASGD and epoch % Logging_interval == 0 and T == 0:\n",
    "                if (len(loss_log) > 0):\n",
    "                    if t > non_monotone_interval and ppl_dev > min(loss_log):\n",
    "                        T = epoch\n",
    "                        optimizer = ASGD_optim(\n",
    "                            modelM.parameters(), lr=lr, weight_decay=weight_decay_Av)\n",
    "                        print(f\"swiching to ASGD at epoch{epoch}\")\n",
    "                        patience = patience_set\n",
    "                loss_log.append(ppl_dev)\n",
    "                t = t+1\n",
    "\n",
    "            # tensorboard logger\n",
    "            log_values(writer, epoch, ppl_dev, exp_name+\"PPL\")\n",
    "\n",
    "            pbar.set_description(\"PPL: %f Patience:%d\" % (ppl_dev, patience))\n",
    "\n",
    "            if ppl_dev < best_ppl:  # the lower, the better\n",
    "                best_ppl = ppl_dev\n",
    "                best_model = copy.deepcopy(modelM).to('cpu')\n",
    "                patience = patience_set\n",
    "            else:\n",
    "                patience -= 1\n",
    "\n",
    "            if patience <= 0:  # Early stopping with patience\n",
    "                break  # Not nice but it keeps the code clean\n",
    "\n",
    "    best_model.to(DEVICE)\n",
    "    final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
    "    print('Test ppl: ', final_ppl)\n",
    "    return final_ppl,modelM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" class LSTM_DROP(nn.Module):\\n    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\\n                 emb_dropout=0.1, n_layers=1):\\n        super(LSTM_DROP, self).__init__()\\n        # Token ids to vectors, we will better see this in the next lab\\n        # matrix vocabulary*dimension of the input\\n        self.embedding = nn.Embedding(\\n            output_size, emb_size, padding_idx=pad_index)\\n        self.drop1 = nn.Dropout(p=emb_dropout)\\n\\n        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\\n        self.rnn = nn.LSTM(emb_size, hidden_size,\\n                           n_layers, bidirectional=False)\\n        self.pad_token = pad_index\\n        # Linear layer to project the hidden layer to our output space\\n        self.drop2 = nn.Dropout(p=emb_dropout)\\n        self.output = nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, input_sequence):\\n        emb = self.embedding(input_sequence)  # sentence\\n        drop1_out = self.drop1(emb)\\n        rnn_out, _ = self.rnn(drop1_out)\\n        drop2_out = self.drop2(rnn_out)\\n        output = self.output(drop2_out).permute(0, 2, 1)\\n        return output \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class LSTM_DROP(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LSTM_DROP, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        # matrix vocabulary*dimension of the input\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, emb_size, padding_idx=pad_index)\n",
    "        self.drop1 = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size,\n",
    "                           n_layers, bidirectional=False)\n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space\n",
    "        self.drop2 = nn.Dropout(p=emb_dropout)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)  # sentence\n",
    "        drop1_out = self.drop1(emb)\n",
    "        rnn_out, _ = self.rnn(drop1_out)\n",
    "        drop2_out = self.drop2(rnn_out)\n",
    "        output = self.output(drop2_out).permute(0, 2, 1)\n",
    "        return output \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" class LSTM_SIMPLE(nn.Module):\\n    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\\n                 emb_dropout=0.1, n_layers=1):\\n        super(LSTM_SIMPLE, self).__init__()\\n        # Token ids to vectors, we will better see this in the next lab\\n        # matrix vocabulary*dimension of the input\\n        self.embedding = nn.Embedding(\\n            output_size, emb_size, padding_idx=pad_index)\\n        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\\n        self.rnn = nn.LSTM(emb_size, hidden_size,\\n                           n_layers, bidirectional=False)\\n        self.pad_token = pad_index\\n        # Linear layer to project the hidden layer to our output space\\n        self.output = nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, input_sequence):\\n        emb = self.embedding(input_sequence)  # sentence\\n        rnn_out, _ = self.rnn(emb)\\n        output = self.output(rnn_out).permute(0, 2, 1)\\n        return output \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class LSTM_SIMPLE(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LSTM_SIMPLE, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        # matrix vocabulary*dimension of the input\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size,\n",
    "                           n_layers, bidirectional=False)\n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)  # sentence\n",
    "        rnn_out, _ = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0, 2, 1)\n",
    "        return output \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "# Increasing the back propagation steps can be seen as a regularization step\n",
    "\n",
    "# With SGD try with an higher learning rate (> 1 for instance)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fdfabc7e2dfda5c0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fdfabc7e2dfda5c0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check the results on tensorboard\n",
    "# NOTE: remember to set the smoothing to zero\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grid_search(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type=\"None\", dropout_type=\"None\", NM_ASGD=False, optimizer=None, n_epochs=100, exp_name=\"Pollo\", ASGD_optim=NTAvSGD, Weight_tying=False):\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    optimizer_class = optimizer\n",
    "    model = None\n",
    "    # Iterate over all combinations of the parameter ranges\n",
    "    for lr, clip, hid_size, emb_size, weight_decay, patience, emb_drop, out_drop in itertools.product(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range):\n",
    "        if Weight_tying and emb_drop !=out_drop:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Training with parameters: lr={lr}, clip={clip}, hid_size={hid_size}, emb_size={emb_size}, weight_decay={weight_decay}, patience={patience}, emb_drop={emb_drop}, out_drop={out_drop}\")\n",
    "\n",
    "        # Initialize the model with current parameters\n",
    "        model = MultiModel(emb_size, hid_size, vocab_len, model_type=model_type,\n",
    "                           pad_index=lang.word2id[\"<pad>\"], dropout_type=dropout_type,\n",
    "                           emb_dropout=emb_drop, out_dropout=out_drop,\n",
    "                           Weight_tying=Weight_tying).to(DEVICE)\n",
    "\n",
    "        model.apply(init_weights)\n",
    "\n",
    "        # Set up the optimizer with current parameters\n",
    "        optimizer_obj = optimizer_class(model.parameters(), lr=lr,\n",
    "                                        weight_decay=weight_decay)\n",
    "\n",
    "        if dropout_type == \"Variational\":\n",
    "            VDROP= True;\n",
    "        else: VDROP = False\n",
    "        # Train the model\n",
    "        accuracy, model = train_model(exp_name+\"/\"+f\"LR={lr}, C={clip}, HS={hid_size}, ES={emb_size}, WD={weight_decay}, P={patience}, E_D={emb_drop}, O_D={out_drop},M_T={model_type},D_T={dropout_type},ASGD={NM_ASGD}, VDROP={VDROP},WT={Weight_tying}\", clip=clip, modelM=model,\n",
    "                                      optimizer=optimizer_obj, NM_ASGD=NM_ASGD, VDROP=VDROP,\n",
    "                                      criterion_eval=criterion_eval,\n",
    "                                      criterion_train=criterion_train, patience_set=patience, weight_decay_Av=weight_decay,\n",
    "                                      n_epochs=n_epochs, exp_name=exp_name,\n",
    "                                      ASGD_optim=ASGD_optim)\n",
    "\n",
    "        # Check if the current model is the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_params = {\n",
    "                'lr': lr,\n",
    "                'clip': clip,\n",
    "                'hid_size': hid_size,\n",
    "                'emb_size': emb_size,\n",
    "                'weight_decay': weight_decay,\n",
    "                'patience': patience,\n",
    "                'emb_drop': emb_drop,\n",
    "                'out_drop': out_drop\n",
    "            }\n",
    "\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "            }, \"models/\"+exp_name+\".model\")\n",
    "\n",
    "    print(f\"Best model parameters: {best_params}\")\n",
    "    print(f\"Best model accuracy: {best_accuracy}\")\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lr = 1\\nclip = 5\\nhid_size = 400\\nemb_size = 250\\npatience = 3\\nweight_decay = .0001\\n\\nmodel = MultiModel(emb_size, hid_size, vocab_len, model_type=\"RNN\",\\n                   pad_index=lang.word2id[\"<pad>\"], dropout_type=\"None\").to(DEVICE)\\n\\nmodel.apply(init_weights)\\noptimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\\ntrain_model(\"LM_RNN_SGD\", modelM=model,\\n            optimizer=optimizer, patience_set=patience,clip=clip,\\n            criterion_train=criterion_train,criterion_eval = criterion_eval) '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" lr = 1\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "patience = 3\n",
    "weight_decay = .0001\n",
    "\n",
    "model = MultiModel(emb_size, hid_size, vocab_len, model_type=\"RNN\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"], dropout_type=\"None\").to(DEVICE)\n",
    "\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LM_RNN_SGD\", modelM=model,\n",
    "            optimizer=optimizer, patience_set=patience,clip=clip,\n",
    "            criterion_train=criterion_train,criterion_eval = criterion_eval) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 386.034632 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  368.63091810206885\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 387.536501 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  375.8707549809068\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 364.914401 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  352.994516929359\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 371.213559 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  357.7792336850881\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 364.813845 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  353.62477385568144\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 372.399378 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  355.96619282554155\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 378.572292 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  364.1622613049077\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 368.830257 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  355.0932445496852\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 372.050661 Patience:3: 100%|██████████| 4/4 [00:42<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  358.41027857362803\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 462.363150 Patience:3: 100%|██████████| 4/4 [00:43<00:00, 10.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  445.24979270006804\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 464.278840 Patience:3: 100%|██████████| 4/4 [01:06<00:00, 16.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  450.0608611160987\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 454.214314 Patience:2: 100%|██████████| 4/4 [01:04<00:00, 16.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  441.2352995326771\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 452.679389 Patience:3: 100%|██████████| 4/4 [00:36<00:00,  9.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  435.0991051347404\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 469.079423 Patience:2: 100%|██████████| 4/4 [00:33<00:00,  8.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  453.33604376453087\n",
      "Training with parameters: lr=1.0, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m emb_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m      8\u001b[0m out_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_size_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_size_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_drop_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_drop_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLM_RNN_SGD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mtrain_grid_search\u001b[0;34m(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type, dropout_type, NM_ASGD, VDROP, optimizer, n_epochs, exp_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer_obj \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     22\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mhid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mout_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check if the current model is the best so far\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(param_string, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval, exp_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mavg_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# The epoch is treated as the k in NT-AvSGD\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mavg_train_loop\u001b[0;34m(data, optimizer, criterion, model, VDROP, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\u001b[38;5;66;03m#data is the list of batches\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VDROP:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_range = [1.0,2.0,3.0]\n",
    "clip_range = [1, 5, 10]\n",
    "hid_size_range = [100, 250, 500]\n",
    "emb_size_range = [100, 250, 500]\n",
    "weight_decay_range = [0.0001, 0.001]\n",
    "patience_range = [3]\n",
    "emb_drop_range = [0]\n",
    "out_drop_range = [0]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    lr_range, clip_range, hid_size_range, emb_size_range,\n",
    "    weight_decay_range, patience_range, emb_drop_range,\n",
    "    out_drop_range, vocab_len, exp_name=\"LM_RNN_SGD\", model_type=\"RNN\", optimizer=optim.SGD, dropout_type=\"None\",n_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 227.514157 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.0682208819467\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 224.122977 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  205.54534527315607\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 219.958884 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  202.57594334611056\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 225.354042 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.3225177153071\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 221.953195 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  204.3492405949697\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 221.446146 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  204.10735333089437\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 225.825093 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.14434560066405\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 225.898250 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.17104266692974\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 225.136839 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.99345027395344\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 226.833923 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  207.4086389997443\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 220.557057 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  203.27759464523834\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 220.669347 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  201.32729244678742\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 223.517595 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  204.6592486693378\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 222.700134 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  204.6898874579707\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 221.737523 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  203.09780505741787\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 227.222422 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  207.80592136961803\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 224.558973 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  205.7196415066298\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.3, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 226.128415 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  207.27285750950608\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 239.416910 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  216.19805408658317\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 236.898563 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  215.71614352867527\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 235.793552 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  215.4145885601564\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 237.708991 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  214.7828709425306\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 235.878708 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  216.60711304473605\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 235.955418 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  215.5352934388295\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 237.159264 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  215.27406430675967\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 235.243364 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  214.21227649692173\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 236.269031 Patience:3: 100%|██████████| 4/4 [00:35<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  214.43729379849245\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=250, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 248.709709 Patience:3:  25%|██▌       | 1/4 [00:17<00:52, 17.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m emb_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m     25\u001b[0m out_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_size_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_size_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_drop_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_drop_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_DROP_AdamW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mtrain_grid_search\u001b[0;34m(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type, dropout_type, NM_ASGD, VDROP, optimizer, n_epochs, exp_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer_obj \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     22\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mhid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mout_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check if the current model is the best so far\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[26], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(param_string, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval, exp_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m sampled_epochs\u001b[38;5;241m.\u001b[39mappend(epoch)\n\u001b[1;32m     44\u001b[0m losses_train\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39masarray(loss)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m---> 45\u001b[0m ppl_dev, loss_dev \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m losses_dev\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39masarray(loss_dev)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 4-9: of NT-AvSGD\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36meval_loop\u001b[0;34m(data, eval_criterion, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# It used to avoid the creation of computational graph\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m#torch.cuda.empty_cache()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m         loss \u001b[38;5;241m=\u001b[39m eval_criterion(output, sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" lr = .002\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "weight_decay=.0001 \"\"\"\n",
    "\"\"\" model = LSTM_DROP(emb_size, hid_size, vocab_len,\n",
    "                  pad_index=lang.word2id[\"<pad>\"]).to(DEVICE) \"\"\"\n",
    "\"\"\" model = MultiModel(emb_size, hid_size, vocab_len, model_type=\"LSTM\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"],dropout_type=\"Normal\").to(DEVICE)\n",
    "\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LSTM_DROP_AdamW\", modelM=model,\n",
    "            optimizer=optimizer, patience_set=patience, clip=clip,\n",
    "            criterion_train=criterion_train, criterion_eval = criterion_eval) \"\"\"\n",
    "\n",
    "\n",
    "lr_range = [.02,.002,.001]\n",
    "clip_range = [1, 5, 10]\n",
    "hid_size_range = [100, 250, 500]\n",
    "emb_size_range = [100, 250, 500]\n",
    "weight_decay_range = [0.0001, 0.001]\n",
    "patience_range = [3]\n",
    "emb_drop_range = [0.1, 0.2, 0.3]\n",
    "out_drop_range = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    lr_range, clip_range, hid_size_range, emb_size_range,\n",
    "    weight_decay_range, patience_range, emb_drop_range,\n",
    "    out_drop_range, vocab_len, exp_name=\"LSTM_DROP_AdamW\", model_type=\"LSTM\", optimizer=optim.AdamW, dropout_type=\"Normal\", n_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 229.124991 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  208.36720312424922\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 223.902258 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.68925456044397\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 221.426062 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  203.04070360725035\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 226.733405 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  206.18938866737108\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 224.033702 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  205.22984994042005\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 224.452930 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  205.02874281819385\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 227.712145 Patience:3: 100%|██████████| 4/4 [00:34<00:00,  8.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  209.51014514850854\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 233.561971 Patience:3:  50%|█████     | 2/4 [00:20<00:20, 10.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m emb_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m     21\u001b[0m out_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_size_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_size_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_drop_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_drop_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_SIMPLE_AdamW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mtrain_grid_search\u001b[0;34m(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type, dropout_type, NM_ASGD, VDROP, optimizer, n_epochs, exp_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer_obj \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     22\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mhid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mout_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Check if the current model is the best so far\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(param_string, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval, exp_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mavg_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# The epoch is treated as the k in NT-AvSGD\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mavg_train_loop\u001b[0;34m(data, optimizer, criterion, model, VDROP, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\u001b[38;5;66;03m#data is the list of batches\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VDROP:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" lr = .002\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "weight_decay = .0001\n",
    "model = MultiModel(emb_size, hid_size, vocab_len, model_type=\"LSTM\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LSTM_SIMPLE_AdamW\", modelM=model, optimizer=optimizer,clip=clip,\n",
    "            criterion_train=criterion_train, criterion_eval=criterion_eval) \"\"\"\n",
    "\n",
    "\n",
    "lr_range = [.02, .002, .001]\n",
    "clip_range = [1, 5, 10]\n",
    "hid_size_range = [100, 250, 500]\n",
    "emb_size_range = [100, 250, 500]\n",
    "weight_decay_range = [0.0001, 0.001]\n",
    "patience_range = [3]\n",
    "emb_drop_range = [0.1, 0.2, 0.3]\n",
    "out_drop_range = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    lr_range, clip_range, hid_size_range, emb_size_range,\n",
    "    weight_decay_range, patience_range, emb_drop_range,\n",
    "    out_drop_range, vocab_len, exp_name=\"LSTM_SIMPLE_AdamW\", model_type=\"LSTM\", optimizer=optim.AdamW, dropout_type=\"Normal\", n_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 LSTM_SIMPLE with NT-AvSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 1036.176119 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  1000.8666328442101\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 997.690517 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.53s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  963.5641263094466\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 983.997665 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.47s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  949.4518474248052\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 1006.860452 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  972.7105745155079\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 990.162177 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.47s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  955.7165720133253\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 982.830063 Patience:3: 100%|██████████| 11/11 [01:32<00:00,  8.44s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  951.1954525624336\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 997.770345 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.49s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  963.0751663333928\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 978.785296 Patience:3: 100%|██████████| 11/11 [01:34<00:00,  8.55s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  945.3003111931621\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.3, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 998.685300 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.50s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  962.9482344576794\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 981.050839 Patience:3: 100%|██████████| 11/11 [01:33<00:00,  8.51s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  946.9742691918989\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 1474.334753 Patience:3:  64%|██████▎   | 7/11 [01:04<00:36,  9.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m emb_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m     27\u001b[0m out_drop_range \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_size_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_size_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_drop_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_drop_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_SIMPLE_NT-AvSGD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mtrain_grid_search\u001b[0;34m(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type, dropout_type, NM_ASGD, VDROP, optimizer, n_epochs, exp_name, ASGD_optim)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer_obj \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     22\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mhid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43memb_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mout_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mASGD_optim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASGD_optim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Check if the current model is the best so far\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(param_string, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval, exp_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mavg_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# The epoch is treated as the k in NT-AvSGD\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mavg_train_loop\u001b[0;34m(data, optimizer, criterion, model, VDROP, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\u001b[38;5;66;03m#data is the list of batches\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VDROP:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" lr = 1\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "weight_decay = .0001\n",
    "non_monotone_interval = 5\n",
    "emb_drop=.1\n",
    "out_drop=.1\n",
    "model = MultiModel(emb_size, hid_size, vocab_len, model_type=\"LSTM\",\n",
    "                         pad_index=lang.word2id[\"<pad>\"], dropout_type=\"None\",\n",
    "                   emb_dropout=emb_drop, out_dropout=out_drop).to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LSTM_SIMPLE_NT-AvSGD\", modelM=model, optimizer=optimizer,\n",
    "            NM_ASGD=True, clip=clip, criterion_train=criterion_train,\n",
    "              non_monotone_interval=non_monotone_interval,\n",
    "              weight_decay_Av = weight_decay,criterion_eval=criterion_eval\n",
    "              ) \"\"\"\n",
    "\n",
    "lr_range = [.02, .002, .001]\n",
    "clip_range = [1, 5, 10]\n",
    "hid_size_range = [100, 250, 500]\n",
    "emb_size_range = [100, 250, 500]\n",
    "weight_decay_range = [0.0001, 0.001]\n",
    "patience_range = [3]\n",
    "emb_drop_range = [0]\n",
    "out_drop_range = [0]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    lr_range, clip_range, hid_size_range, emb_size_range,\n",
    "    weight_decay_range, patience_range, emb_drop_range,\n",
    "    out_drop_range, vocab_len, exp_name=\"LSTM_SIMPLE_NT-AvSGD\",\n",
    "      optimizer=optim.SGD, dropout_type=\"None\", model_type= \"LSTM\",n_epochs=12, NM_ASGD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 254.694746 Patience:3:  13%|█▎        | 13/99 [03:23<22:25, 15.64s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiModel(emb_size, hid_size, vocab_len, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                    pad_index\u001b[38;5;241m=\u001b[39mlang\u001b[38;5;241m.\u001b[39mword2id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m], dropout_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m                    emb_dropout\u001b[38;5;241m=\u001b[39memb_drop, out_dropout\u001b[38;5;241m=\u001b[39mout_drop)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_SIMPLE_AverageOfGradients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnon_monotone_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_monotone_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mASGD_optim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAverageOfGradientsSGD\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(exp_name, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m non_monotone_interval \u001b[38;5;129;01mand\u001b[39;00m ppl_dev \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mmin\u001b[39m(loss_log):\n\u001b[1;32m     49\u001b[0m     T \u001b[38;5;241m=\u001b[39m epoch\n\u001b[0;32m---> 50\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mASGD_optim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay_Av\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswiching to ASGD at epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     patience \u001b[38;5;241m=\u001b[39m patience_set\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/labs/model.py:163\u001b[0m, in \u001b[0;36mAverageOfGradientsSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28msuper\u001b[39m(AverageOfGradientsSGD, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    158\u001b[0m     params, lr, momentum, dampening, weight_decay, nesterov\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAverageOfGradientsSGD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 163\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/optim/sgd.py:14\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dampening\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     12\u001b[0m              weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, maximize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m              differentiable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m momentum \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "\"\"\" lr = 1\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "weight_decay = .0001\n",
    "non_monotone_interval = 5\n",
    "emb_drop = .1\n",
    "out_drop = .1\n",
    "\n",
    "model = MultiModel(emb_size, hid_size, vocab_len, model_type=\"LSTM\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"], dropout_type=\"None\",\n",
    "                   emb_dropout=emb_drop, out_dropout=out_drop).to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LSTM_SIMPLE_AverageOfGradients\", modelM=model, optimizer=optimizer,\n",
    "            NM_ASGD=True, clip=clip, criterion_train=criterion_train,\n",
    "            non_monotone_interval=non_monotone_interval,\n",
    "            weight_decay_Av=weight_decay, criterion_eval=criterion_eval,\n",
    "            ASGD_optim=AverageOfGradientsSGD\n",
    "            ) \"\"\"\n",
    "\n",
    "Lr = [.02, .002, .001]\n",
    "Clip = [1, 5, 10]\n",
    "Hid_size = [100, 250, 500]\n",
    "Emb_size = [100, 250, 500]\n",
    "Weighy_decay = [0.0001, 0.001]\n",
    "Patience_range = [3]\n",
    "Emb_drop = [0.1, 0.2, 0.3]\n",
    "Out_drop = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    Lr, Clip, Hid_size, Emb_size,\n",
    "    Weighy_decay, Patience_range, Emb_drop,\n",
    "    Out_drop, vocab_len, exp_name=\"LSTM_SIMPLE_NT-AverageOfGradientsSGD\",\n",
    "    optimizer=optim.SGD, dropout_type=\"None\", n_epochs=12, NM_ASGD=True,\n",
    "    ASGD_optim=AverageOfGradientsSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 V_DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 109336.353031 Patience:7:   4%|▍         | 4/99 [01:10<27:55, 17.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model_class\u001b[38;5;241m.\u001b[39mapply(init_weights)\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model_class\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     18\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_SIMPLE_NTASGD_VDROP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(exp_name, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, device, NM_ASGD, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval)\u001b[0m\n\u001b[1;32m     35\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mavg_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# The epoch is treated as the k in NT-AvSGD\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mavg_train_loop\u001b[0;34m(data, optimizer, criterion, model, VDROP, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\u001b[38;5;66;03m#data is the list of batches\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VDROP:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"lr = 5\n",
    "clip = 5\n",
    "hid_size = 400\n",
    "emb_size = 250\n",
    "weight_decay = .0001\n",
    "patience = 8\n",
    "non_monotone_interval=5\n",
    "\n",
    " model_class = MultiModel(emb_size, hid_size, vocab_len,model_type=\"LSTM\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"],dropout_type=\"Variational\",\n",
    "                         emb_dropout=emb_drop, out_dropout=out_drop).to(DEVICE)\n",
    "model_class.apply(init_weights)\n",
    "optimizer = optim.SGD(model_class.parameters(), lr=lr,\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "train_model(\"LSTM_SIMPLE_NTASGD_VDROP\", clip=clip, modelM=model_class,emb_size, hid_size, vocab_len,model_type=\"LSTM\",\n",
    "                   pad_index=lang.word2id[\"<pad>\"],dropout_type=\"Variational\",\n",
    "                         emb_dropout=emb_drop, out_dropout=out_drop\n",
    "            criterion_eval=criterion_eval,\n",
    "            criterion_train=criterion_train,\n",
    "            patience_set=patience,weight_decay_Av=weight_decay) \"\"\"\n",
    "\n",
    "\n",
    "Lr = [.02, .002, .001]\n",
    "Clip = [1, 5, 10]\n",
    "Hid_size = [100, 250, 500]\n",
    "Emb_size = [100, 250, 500]\n",
    "Weighy_decay = [0.0001, 0.001]\n",
    "Patience_range = [3]\n",
    "Emb_drop = [0.1, 0.2, 0.3]\n",
    "Out_drop = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    Lr, Clip, Hid_size, Emb_size,\n",
    "    Weighy_decay, Patience_range, Emb_drop,\n",
    "    Out_drop, vocab_len,  exp_name=\"LSTM_SIMPLE_NTASGD_VDROP\",\n",
    "    optimizer=optim.SGD, dropout_type=\"Variational\", model_type=\"LSTM\", n_epochs=EPOCHS, NM_ASGD=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Weight_tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-use-shared-weights-in-different-layers-of-a-model/71263\n",
    "\n",
    "\n",
    "class LSTM_VDROP_Weight(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LSTM_VDROP_Weight, self).__init__()\n",
    "        assert (emb_size == hidden_size),\"For Weight Tying emb_size and hidden size must be the same\"\n",
    "        # Token ids to vectors, we will better see this in the next lab\n",
    "        # matrix vocabulary*dimension of the input\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, emb_size, padding_idx=pad_index)\n",
    "        self.drop1 = V_Dropout(p=emb_dropout)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size,\n",
    "                            n_layers, bidirectional=False)\n",
    "        self.pad_token = pad_index\n",
    "        self.drop2 = V_Dropout(p=out_dropout)\n",
    "\n",
    "        # Linear layer to project the hidden layer to our output space\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.output.weight.data = self.embedding.weight.data\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)  # sentence\n",
    "        drop1_out = self.drop1(emb)\n",
    "        lstm_out, _ = self.lstm(drop1_out)\n",
    "        drop2_out = self.drop2(lstm_out)\n",
    "        output = self.output(drop2_out).permute(0, 2, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset_dropout(self, input_sample):\n",
    "        emb = self.embedding(input_sample)\n",
    "        self.drop1.generate_mask(emb.to(DEVICE))\n",
    "        drop1_out = self.drop1(emb)\n",
    "        lstm_out, _ = self.lstm(drop1_out)\n",
    "        self.drop2.generate_mask(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 7271.472979 Patience:3: 100%|██████████| 4/4 [00:44<00:00, 11.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  7203.020650022584\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 7266.561004 Patience:3: 100%|██████████| 4/4 [00:44<00:00, 11.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  7197.654075608684\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.1, out_drop=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 7258.524982 Patience:3: 100%|██████████| 4/4 [00:44<00:00, 11.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ppl:  7190.80653680236\n",
      "Training with parameters: lr=0.02, clip=1, hid_size=100, emb_size=100, weight_decay=0.0001, patience=3, emb_drop=0.2, out_drop=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 8524.265331 Patience:3:  50%|█████     | 2/4 [00:26<00:26, 13.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m Emb_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m     29\u001b[0m Out_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mClip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEmb_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWeighy_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPatience_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEmb_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOut_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM_NTASGD_VDROP_WEIGHT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVariational\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWeight_tying\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m, in \u001b[0;36mtrain_grid_search\u001b[0;34m(lr_range, clip_range, hid_size_range, emb_size_range, weight_decay_range, patience_range, emb_drop_range, out_drop_range, vocab_len, model_type, dropout_type, NM_ASGD, optimizer, n_epochs, exp_name, ASGD_optim, Weight_tying)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: VDROP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m accuracy, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLR=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, C=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclip\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, HS=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhid_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, ES=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43memb_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, WD=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mweight_decay\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, P=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpatience\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, E_D=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43memb_drop\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, O_D=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_drop\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,M_T=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,D_T=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdropout_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,ASGD=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, VDROP=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mVDROP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,WT=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mWeight_tying\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNM_ASGD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNM_ASGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcriterion_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_Av\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mASGD_optim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASGD_optim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Check if the current model is the best so far\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(param_string, n_epochs, patience_set, losses_train, losses_dev, sampled_epochs, best_ppl, best_model, modelM, optimizer, NM_ASGD, ASGD_optim, Logging_interval, non_monotone_interval, lr, VDROP, clip, criterion_train, weight_decay_Av, criterion_eval, exp_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mavg_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcriterion_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# The epoch is treated as the k in NT-AvSGD\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mavg_train_loop\u001b[0;34m(data, optimizer, criterion, model, VDROP, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m number_of_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data:\u001b[38;5;66;03m#data is the list of batches\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VDROP:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programmi/ProgettiGit/NLU-Assignemnts/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data, pad_token)\u001b[0m\n\u001b[1;32m     28\u001b[0m source, _ \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m target, lengths \u001b[38;5;241m=\u001b[39m merge(new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     33\u001b[0m new_item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" #to iterate on\n",
    "lr = 1.0\n",
    "clip = 5\n",
    "hid_size = 250\n",
    "emb_size = 250\n",
    "weight_decay = .0001\n",
    "patience = 8\n",
    "emb_drop = 0.2\n",
    "out_drop = 0.2\n",
    "#end to iterate on\n",
    "\n",
    "model = LSTM_VDROP_Weight(emb_size, hid_size, vocab_len,\n",
    "                          pad_index=lang.word2id[\"<pad>\"],\n",
    "                          emb_dropout=emb_drop, out_dropout=out_drop).to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "train_model(\"LSTM_NTASGD_VDROP_WEIGHT\", clip=clip, modelM=model,\n",
    "            optimizer=optimizer, NM_ASGD=True, VDROP=True,\n",
    "            criterion_eval=criterion_eval,\n",
    "            criterion_train=criterion_train, patience_set=patience, weight_decay_Av=weight_decay) \"\"\"\n",
    "\n",
    "Lr = [.02, .002, .001]\n",
    "Clip = [1, 5, 10]\n",
    "Hid_size = [100, 250, 500]\n",
    "Emb_size = [100, 250, 500]\n",
    "Weighy_decay = [0.0001, 0.001]\n",
    "Patience_range = [3]\n",
    "Emb_drop = [0.1, 0.2, 0.3]\n",
    "Out_drop = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_model, best_params = train_grid_search(\n",
    "    Lr, Clip, Hid_size, Emb_size,\n",
    "    Weighy_decay, Patience_range, Emb_drop,\n",
    "    Out_drop, vocab_len,  exp_name=\"LSTM_NTASGD_VDROP_WEIGHT\",\n",
    "    optimizer=optim.SGD, dropout_type=\"Variational\", model_type=\"LSTM\", n_epochs=EPOCHS, NM_ASGD=True, Weight_tying=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%rm -r runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
